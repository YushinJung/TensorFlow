{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "individual-spread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-health",
   "metadata": {},
   "source": [
    "# Lab 9 - 1 따라하기\n",
    "## Data Handle \n",
    "* data load \n",
    "* pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "widespread-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 2), (None, 1)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "# make as dataset format\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "dataset = dataset.batch(len(x_data))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(feature, label):\n",
    "    feature = tf.cast(feature, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-cradle",
   "metadata": {},
   "source": [
    "## 모델 설정\n",
    "* first layer will have logistic regression format\n",
    "* second layer will have another logistic regression format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "unlikely-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.uniform(shape=(2,2), minval = -1, maxval = 1), name = 'Weight1')\n",
    "B1 = tf.Variable(tf.random.uniform(shape=(1,2), minval = -1, maxval = 1), name = 'Bias1')\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(2,1), minval = -1, maxval = 1), name = 'Weight2')\n",
    "B2 = tf.Variable(tf.random.uniform(shape=(1,1), minval = -1, maxval = 1), name = 'Bias2')\n",
    "\n",
    "variables = [W1, B1, W2, B2]\n",
    "\n",
    "def hypothesis_fn(feature):\n",
    "    y1 = tf.matmul(feature, W1) + B1\n",
    "    layer1 = tf.sigmoid(y1)\n",
    "    y2 = tf.matmul(layer1, W2) + B2\n",
    "    layer2 = tf.sigmoid(y2)\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-sweet",
   "metadata": {},
   "source": [
    "## loss\n",
    "* Let's just use binary classification entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accepting-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(predicted, truth):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = bce(truth, predicted)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-yugoslavia",
   "metadata": {},
   "source": [
    "## training\n",
    "* I'll use SGD (actually I don't know the difference between the adam, let's figure out later)\n",
    "* learning rate will be decreased while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "normal-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = './9-3-xor'\n",
    "writer = tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "norman-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_usingLRDecay = True\n",
    "def learning_schedule(flag_usingLRDecay, start_lr = 0.1, decay_steps = 1000, decay_rate = 0.9):\n",
    "    if flag_usingLRDecay:\n",
    "        learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=start_lr, decay_steps = decay_steps, decay_rate = decay_rate, \n",
    "                                                                       staircase=False, name=None)\n",
    "    else:\n",
    "        learning_rate = start_lr\n",
    "    return learning_rate\n",
    "\n",
    "learning_rate = learning_schedule(flag_usingLRDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "occupied-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(feature, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # get loss\n",
    "        loss = loss_fn(hypothesis_fn(feature), label)\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "instructional-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def training_fn(dataset, epoch = 15000, verbose = 1000):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    for step in tqdm(range(epoch)):\n",
    "    # get batch\n",
    "        for feature, label in dataset:\n",
    "            feature, label = preprocess_fn(feature, label)\n",
    "            # grad\n",
    "            grads = grad_fn(feature, label)\n",
    "            # update values\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram('weight1', W1, step = step)\n",
    "                tf.summary.histogram('bias1', B1, step = step)\n",
    "                tf.summary.histogram('weight2', W2, step = step)\n",
    "                tf.summary.histogram('bias2', B2, step = step)\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', optimizer._decayed_lr(tf.float32), step = step)\n",
    "            tf.summary.scalar('loss', loss_fn(hypothesis_fn(feature), label), step = step)\n",
    "        if step % verbose == 0 :\n",
    "            print(f'step-{step}\\tloss-{loss_fn(hypothesis_fn(feature), label)}\\tlearning rate-{optimizer._decayed_lr(tf.float32)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "welsh-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbd5ab6ce3047629371ee789fdc32cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step-0\tloss-0.7783117294311523\tlearning rate-0.09998946636915207\n",
      "step-1000\tloss-0.6929596662521362\tlearning rate-0.0899905189871788\n",
      "step-2000\tloss-0.6917161345481873\tlearning rate-0.0809914693236351\n",
      "step-3000\tloss-0.6857339143753052\tlearning rate-0.07289231568574905\n",
      "step-4000\tloss-0.6537238359451294\tlearning rate-0.06560308486223221\n",
      "step-5000\tloss-0.5666872262954712\tlearning rate-0.05904277041554451\n",
      "step-6000\tloss-0.47874003648757935\tlearning rate-0.05313849449157715\n",
      "step-7000\tloss-0.43033552169799805\tlearning rate-0.047824643552303314\n",
      "step-8000\tloss-0.4059346318244934\tlearning rate-0.043042175471782684\n",
      "step-9000\tloss-0.3924615979194641\tlearning rate-0.038737956434488297\n"
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "training_fn(dataset, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-frost",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hungry-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "southern-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables - [<tf.Variable 'Weight1:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-6.269986, -3.62093 ],\n",
      "       [-6.314083, -3.624776]], dtype=float32)>, <tf.Variable 'Bias1:0' shape=(1, 2) dtype=float32, numpy=array([[2.2659688, 5.2387614]], dtype=float32)>, <tf.Variable 'Weight2:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-8.256178 ],\n",
      "       [ 7.2969365]], dtype=float32)>, <tf.Variable 'Bias2:0' shape=(1, 1) dtype=float32, numpy=array([[-3.081015]], dtype=float32)>]\n",
      "result of trained 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'variables - {variables}')\n",
    "for feature, label in dataset:\n",
    "    feature, label = preprocess_fn(feature, label)\n",
    "    print(f'result of trained {accuracy_fn(hypothesis_fn(feature), label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-shield",
   "metadata": {},
   "source": [
    "## Save Model to use Later\n",
    "* 특별한 save function을 설정하지 않았기 때문에, 간단히 weight 만 저장하도록 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-fifth",
   "metadata": {},
   "source": [
    "checkpoint 를 쓰라고 하는데, 일단은 패스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-technician",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
