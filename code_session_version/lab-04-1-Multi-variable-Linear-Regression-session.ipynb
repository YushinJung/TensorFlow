{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 03 - Multi-variable Linear Regression\n",
    "\n",
    "<img width=\"200\" src=\"https://i.imgur.com/hbPVe1T.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis and Cost\n",
    "\n",
    "$$ H(x) = Wx + b $$ \n",
    "\n",
    "$$ cost(W, b)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (H{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "# Simplifed hypothesis\n",
    "\n",
    "$$ H(x) = Wx $$ \n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "b를 W 행렬에 넣어 표현할 수 있기 때문에 생략 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "W = -1, cost(W) = 18.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (-1 * 1 - 1)^2 + (-1 * 2 - 2)^2 + (-1 * 3 - 3)^2) $$\n",
    "\n",
    "W = 0, cost(W) = 4.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (0 * 1 - 1)^2 + (0 * 2 - 2)^2 + (0 * 3 - 3)^2) $$\n",
    "\n",
    "W = 1, cost(W) = 0\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (1 * 1 - 1)^2 + (1 * 2 - 2)^2 + (1 * 3 - 3)^2) $$\n",
    "\n",
    "W = 2, cost(W) = 4.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (2 * 1 - 1)^2 + (2 * 2 - 2)^2 + (2 * 3 - 3)^2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function in pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"%6.3f | %10.5f\" % (feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def hypothesis(X, W):\n",
    "    return X * W\n",
    "cost = lambda W: tf.reduce_mean(tf.square(hypothesis(X, W) - Y))\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost(feed_W)\n",
    "    # curr_cost = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    # print(\"%6.3f | %10.5f\" % (feed_W, curr_cost[0]))\n",
    "    print('{:6.3f} | {:10.5f}'.format(feed_W, curr_cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(0.0)\n",
    "\n",
    "@tf.function\n",
    "def hypothesis(X, W):\n",
    "    return X * W\n",
    "\n",
    "cost = lambda W: tf.reduce_mean(tf.square(hypothesis(X, W) - Y))\n",
    "\n",
    "W_val = np.linspace(-3, 5, num=30)\n",
    "cost_val = []\n",
    "for feed_W in W_val:\n",
    "    curr_cost  = cost(feed_W)\n",
    "    cost_val.append(curr_cost.numpy())\n",
    "\n",
    "plt.plot(W_val, cost_val, \"ro\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to minimize cost?\n",
    "* 현재 데이터 X와 Y에 대해 W가 1일 때 cost 가 가장 작다\n",
    "* cost 가 최소가 되는 W를 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent algorithm\n",
    "* Minimize cost function\n",
    "* used many minimization problems\n",
    "* For a given cost (W, b), it will find W, b to minimize cost\n",
    "* It can be applied to more general function: cost (w1, w2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does it work?\n",
    "* Start with initial guesses\n",
    " * Start at 0,0 (or any other value)\n",
    " * Keeping changing $W$ and $b$ a little bit to try and reduce $cost(W,b)$\n",
    "* Each time you change the parameters, you select the gradient which reduces $cost(W,b)$ the most possible \n",
    "* Repeat\n",
    "* Do so until you converge to a local minimum\n",
    "* Has an interesting property\n",
    " * Where you start can determine which minimum you end up\n",
    "\n",
    "http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "* m 혹은 2m 나누는 것이 cost 최소화에 영향 없음\n",
    "* 제곱을 미분할 때, 2가 앞으로 나오면서 공식이 단순하게 되는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "$$ cost(W)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "$$ W:=W - \\alpha\\frac{ \\partial } {\\partial W } cost(W) $$\n",
    "\n",
    "* W = W - 변화량\n",
    "* 변화량 = 현 위치(W)에서 비용곡선의 기울기(=미분값) X $\\alpha$ <br> $\\alpha$ : learning rate (시도 간격)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "\n",
    "$$ W:=W - \\alpha\\frac{ \\partial } {\\partial W } \\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } } $$\n",
    "\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ 2m } \\sum _{ i=1 }^{ m }{ { 2(W{ x }^{ i }-y^{ i } })x^{ i } }  $$\n",
    "\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { (W{ x }^{ i }-y^{ i } })x^{ i } }  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent algorithm\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { (W{ x }^{ i }-y^{ i } })x^{ i } }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex function\n",
    "<img width=\"40%\" src=\"http://i.imgur.com/TSKliup.png\" >\n",
    "<img width=\"40%\" src=\"http://i.imgur.com/GyCwshy.png\" >\n",
    "\n",
    "Gradient descent algorithm을 사용하려면, 비용함수 cost(W,b)가 Convex function 이어야 한다\n",
    "\n",
    "http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([26.489502], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([26.234608], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1], -1000., 1000.))\n",
    "print(W)\n",
    "X = tf.Variable(1.0)\n",
    "Y = tf.Variable(1.0)\n",
    "def hypothesis(X):\n",
    "    return tf.multiply(W,X)\n",
    "\n",
    "def cost(hypothesis, X, Y):\n",
    "    return tf.reduce_mean(tf.square(hypothesis(X) - Y))\n",
    "\n",
    "def descent(W, X, Y):\n",
    "    mean = tf.reduce_mean(tf.multiply(tf.subtract(tf.multiply(W, X), Y), X))\n",
    "    descent = tf.subtract(W, tf.multiply(tf.constant(0.01), mean))\n",
    "    return descent\n",
    "\n",
    "update = W.assign(descent(W, X, Y))\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 3873.476562500000000 | 24.392011642456055\n",
      "  100 | 0.167321950197220 | 1.676013946533203\n",
      "  200 | 0.166666656732559 | 1.666670441627502\n",
      "  300 | 0.166666656732559 | 1.666667461395264\n",
      "  400 | 0.166666656732559 | 1.666667461395264\n",
      "  500 | 0.166666656732559 | 1.666667461395264\n",
      "  600 | 0.166666656732559 | 1.666667461395264\n",
      "  700 | 0.166666656732559 | 1.666667461395264\n",
      "  800 | 0.166666656732559 | 1.666667461395264\n",
      "  900 | 0.166666656732559 | 1.666667461395264\n",
      " 1000 | 0.166666656732559 | 1.666667461395264\n",
      " 1100 | 0.166666656732559 | 1.666667461395264\n",
      " 1200 | 0.166666656732559 | 1.666667461395264\n",
      " 1300 | 0.166666656732559 | 1.666667461395264\n",
      " 1400 | 0.166666656732559 | 1.666667461395264\n",
      " 1500 | 0.166666656732559 | 1.666667461395264\n",
      " 1600 | 0.166666656732559 | 1.666667461395264\n",
      " 1700 | 0.166666656732559 | 1.666667461395264\n",
      " 1800 | 0.166666656732559 | 1.666667461395264\n",
      " 1900 | 0.166666656732559 | 1.666667461395264\n"
     ]
    }
   ],
   "source": [
    "for step in range(2000):\n",
    "    W.assign(descent(W, x_data, y_data))\n",
    "    cResult = cost(hypothesis, x_data, y_data)\n",
    "    wResult = W\n",
    "    if step % 100 == 0:\n",
    "        print('%5d | %.15f | %.15f' %(step, cResult, wResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "tf.Tensor([8.333338], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.166669], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('-' * 50)\n",
    "print(hypothesis(X = 5))\n",
    "print(hypothesis(X=2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way using gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | [-45.64668] | 23237.73828125\n",
      "1000 | [1.6666663] | 0.1666666716337204\n",
      "2000 | [1.6666663] | 0.1666666716337204\n",
      "3000 | [1.6666663] | 0.1666666716337204\n",
      "4000 | [1.6666663] | 0.1666666716337204\n",
      "5000 | [1.6666663] | 0.1666666716337204\n",
      "6000 | [1.6666663] | 0.1666666716337204\n",
      "7000 | [1.6666663] | 0.1666666716337204\n",
      "8000 | [1.6666663] | 0.1666666716337204\n",
      "9000 | [1.6666663] | 0.1666666716337204\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1], -1000., 1000.))\n",
    "X = tf.Variable(0.0)\n",
    "Y = tf.Variable(0.0)\n",
    "\n",
    "learning_rate = tf.constant(0.01)\n",
    "\n",
    "for i in range(10000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data\n",
    "        cost = tf.reduce_mean(tf.square(tf.subtract(hypothesis,  y_data)))\n",
    "    (W_grad) = tape.gradient(cost, [W])\n",
    "    W.assign_sub(learning_rate * W_grad[0])\n",
    "    if i % 1000 == 0 :\n",
    "        print(f'{i} | {W.numpy()} | {cost.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, Y can not be expressed as `ax = y` , it needs to be `ax + b = y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Liner regression Summary\n",
    "\n",
    "## 1) Hypothesis \n",
    "\n",
    "$$ H(x) = Wx + b $$\n",
    "\n",
    "## 2) Cost function\n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "## 3) Gradient descent\n",
    "\n",
    "$$ W := W-\\alpha \\frac { \\partial  }{ \\partial W } cost(W) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Multi-variable linear regression\n",
    "Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142\n",
    "\n",
    "\n",
    "Test Scores for General Psychology ( https://goo.gl/g2T8Kp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "## dot product(=scalar product, 내적)\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" >\n",
    "\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-feature regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = w x + b $$\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = \\underline{w_1 x_1 + w_2 x_2 + w_3 x_3} + b $$\n",
    "\n",
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$ \n",
    "\n",
    "$$ \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ WX $$ (W, X 는 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "$$ = b + w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "$$ = \\begin{pmatrix} b & x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \\\\ w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ = XW $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix \n",
    "\n",
    "### Many x instances\n",
    "\n",
    "$$ \\begin{pmatrix} x_{ 11 } & x_{ 12 } & x_{ 13 } \\\\ x_{ 21 } & x_{ 22 } & x_{ 23 } \\\\ x_{ 31 } & x_{ 32 } & x_{ 33 }\\\\ x_{ 41 } & x_{ 42 } & x_{ 43 }\\\\ x_{ 51 } & x_{ 52 } & x_{ 53 }\\end{pmatrix} \\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix}=\\begin{pmatrix} x_{ 11 }w_{ 1 }+x_{ 12 }w_{ 2 }+x_{ 13 }w_{ 3 } \\\\ x_{ 21 }w_{ 1 }+x_{ 22 }w_{ 2 }+x_{ 23 }w_{ 3 }\\\\ x_{ 31 }w_{ 1 }+x_{ 32 }w_{ 2 }+x_{ 33 }w_{ 3 } \\\\ x_{ 41 }w_{ 1 }+x_{ 42 }w_{ 2 }+x_{ 43 }w_{ 3 } \\\\ x_{ 51 }w_{ 1 }+x_{ 52 }w_{ 2 }+x_{ 53 }w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ [5, 3] \\cdot [3, 1] = [5, 1] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "5는 데이터(instance)의 수, 3은 변수(feature)의 수, 1은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix (n output)\n",
    "\n",
    "$$ [n, 3] \\cdot [?, ?] = [n, 2] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "* n은 데이터(instance)의 개수, 2는 결과 값의 개수로 주어진다.\n",
    "* 이때, W [?, ?] ⇒ [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WX vs XW\n",
    "\n",
    "### Theory (Lecture) :\n",
    " $$ H(x) = Wx + b  $$\n",
    "\n",
    "### TensorFlow (Implementation) :\n",
    "\n",
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables)\n",
    "\n",
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "b  = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "\n",
    "# hypothesis = W * X + b\n",
    "@tf.function\n",
    "def hypothesis(W1, W2, x1_data, x2_data, b):\n",
    "    return W1 * x1_data + W2 * x2_data + b\n",
    "cost = lambda : tf.reduce_mean(tf.square(tf.subtract(hypothesis(W1, W2, x1_data, x2_data, b),y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   | 0.000000000780892 | 0.999983 | 0.999979 | 0.000066\n",
      "200   | 0.000000000001776 | 0.999999 | 0.999999 | 0.000003\n",
      "300   | 0.000000000000023 | 1.000000 | 1.000000 | 0.000000\n",
      "400   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "500   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "600   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "700   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "800   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "900   | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1000  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1100  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1200  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1300  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1400  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1500  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1600  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1700  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1800  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "1900  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n",
      "2000  | 0.000000000000014 | 1.000000 | 1.000000 | 0.000000\n"
     ]
    }
   ],
   "source": [
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(rate)\n",
    "\n",
    "for step in range(1,2001):\n",
    "    optimizer.minimize(cost, var_list = [W1, W2, b])\n",
    "    if step % 100 == 0:\n",
    "        print(\"%-5d | %.15f | %f | %f | %f\" % (step, cost().numpy(), W1.numpy(), W2.numpy(),b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   | 0.000002354861635 | [[0.99904376 0.99886537]] | 0.003638 \n",
      "200   | 0.000000004782930 | [[0.99995685 0.99994886]] | 0.000164 \n",
      "300   | 0.000000000009663 | [[0.9999981 0.9999977]] | 0.000007 \n",
      "400   | 0.000000000000023 | [[0.9999999 0.9999999]] | 0.000000 \n",
      "500   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "600   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "700   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "800   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "900   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1000  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1100  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1200  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1300  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1400  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1500  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1600  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1700  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1800  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1900  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "2000  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "\n",
    "@tf.function\n",
    "def hypothesis(W, b, x_data):\n",
    "    return tf.matmul(W, x_data) + b     # [1, 2] * [2, 5] = [1, 5]\n",
    "\n",
    "cost = lambda: tf.reduce_mean(tf.square(tf.subtract(hypothesis(W, b, x_data), y_data)))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(rate)\n",
    "for step in range(1,2001):\n",
    "    optimizer.minimize(cost, var_list=[W, b])\n",
    "    if step % 100 == 0:\n",
    "        print(\"%-5d | %.15f | %s | %f \" % (step, cost().numpy(), W.numpy(), b.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50    | 0.002462390344590 | [[0.11763684 0.96907663 0.9633096 ]] | 0.000000 \n",
      "100   | 0.000111080029455 | [[0.0249852  0.99343216 0.9922072 ]] | 0.000000 \n",
      "150   | 0.000005010932000 | [[0.00530662 0.998605   0.9983449 ]] | 0.000000 \n",
      "200   | 0.000000225986781 | [[0.0011271  0.99970376 0.99964845]] | 0.000000 \n",
      "250   | 0.000000010182873 | [[2.3936984e-04 9.9993712e-01 9.9992532e-01]] | 0.000000 \n",
      "300   | 0.000000000460855 | [[5.0766437e-05 9.9998659e-01 9.9998415e-01]] | 0.000000 \n",
      "350   | 0.000000000020455 | [[1.0793168e-05 9.9999720e-01 9.9999660e-01]] | 0.000000 \n",
      "400   | 0.000000000001012 | [[2.2768579e-06 9.9999934e-01 9.9999928e-01]] | 0.000000 \n",
      "450   | 0.000000000000094 | [[5.745497e-07 9.999998e-01 9.999998e-01]] | 0.000000 \n",
      "500   | 0.000000000000048 | [[1.8831179e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "550   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "600   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "650   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "700   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "750   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "800   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "850   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "900   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "950   | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n",
      "1000  | 0.000000000000048 | [[1.6446995e-07 9.9999994e-01 9.9999994e-01]] | 0.000000 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "# 갯수가 같아야 하므로 b를 리스트로 처리\n",
    "\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], \n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
    "\n",
    "hypothesis = lambda: tf.matmul(W, x_data) # b가 없다\n",
    "\n",
    "cost = lambda: tf.reduce_mean(tf.square(tf.subtract(hypothesis(), y_data)))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.keras.optimizers.SGD(rate)\n",
    "\n",
    "for step in range(1,1001):\n",
    "    optimizer.minimize(cost, var_list = [W])\n",
    "    if step % 50 == 0:\n",
    "        # without b\n",
    "        print(\"%-5d | %.15f | %s | %f \" % (step, cost().numpy(), W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variable linear regression\n",
    "\n",
    "version didn't changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0: Cose=2667.6282 Prediction:  [105.34478  129.50781  125.94277  139.99611   97.017456]\n",
      " 100: Cose=  3.8105 Prediction:  [151.11192 184.57016 180.1694  199.04204 139.03413]\n",
      " 200: Cose=  3.7618 Prediction:  [151.08475 184.58958 180.16211 199.02852 139.06659]\n",
      " 300: Cose=  3.7151 Prediction:  [151.05843 184.60847 180.15512 199.01526 139.09831]\n",
      " 400: Cose=  3.6701 Prediction:  [151.03287 184.62682 180.14833 199.00217 139.12927]\n",
      " 500: Cose=  3.6269 Prediction:  [151.0081  184.64462 180.1418  198.98927 139.15953]\n",
      " 600: Cose=  3.5852 Prediction:  [150.9841  184.66193 180.1355  198.97658 139.1891 ]\n",
      " 700: Cose=  3.5450 Prediction:  [150.96082 184.6787  180.1294  198.96404 139.21799]\n",
      " 800: Cose=  3.5063 Prediction:  [150.93826 184.695   180.12355 198.9517  139.24623]\n",
      " 900: Cose=  3.4689 Prediction:  [150.91638 184.7108  180.11787 198.9395  139.27379]\n",
      "1000: Cose=  3.4329 Prediction:  [150.89517 184.72614 180.1124  198.92747 139.30074]\n",
      "1100: Cose=  3.3981 Prediction:  [150.87462 184.74104 180.10715 198.91562 139.32707]\n",
      "1200: Cose=  3.3644 Prediction:  [150.85472 184.7555  180.10208 198.90393 139.35283]\n",
      "1300: Cose=  3.3319 Prediction:  [150.83545 184.76955 180.0972  198.89241 139.378  ]\n",
      "1400: Cose=  3.3004 Prediction:  [150.81677 184.78314 180.0925  198.88101 139.40262]\n",
      "1500: Cose=  3.2700 Prediction:  [150.79869 184.79634 180.088   198.86978 139.42668]\n",
      "1600: Cose=  3.2405 Prediction:  [150.78116 184.80917 180.08365 198.85869 139.4502 ]\n",
      "1700: Cose=  3.2119 Prediction:  [150.76419 184.8216  180.07947 198.84772 139.4732 ]\n",
      "1800: Cose=  3.1842 Prediction:  [150.74779 184.83365 180.07547 198.83693 139.49571]\n",
      "1900: Cose=  3.1573 Prediction:  [150.73192 184.84537 180.07162 198.82626 139.51773]\n",
      "2000: Cose=  3.1312 Prediction:  [150.71652 184.85667 180.0679  198.81569 139.53925]\n",
      "2100: Cose=  3.1058 Prediction:  [150.70163 184.86768 180.06435 198.80527 139.5603 ]\n",
      "2200: Cose=  3.0811 Prediction:  [150.68726 184.87836 180.06096 198.79497 139.58092]\n",
      "2300: Cose=  3.0572 Prediction:  [150.67331 184.88867 180.05768 198.78479 139.60106]\n",
      "2400: Cose=  3.0338 Prediction:  [150.65987 184.8987  180.05458 198.77477 139.62082]\n",
      "2500: Cose=  3.0111 Prediction:  [150.64684 184.90839 180.05157 198.7648  139.6401 ]\n",
      "2600: Cose=  2.9889 Prediction:  [150.63426 184.91782 180.04872 198.75499 139.65901]\n",
      "2700: Cose=  2.9673 Prediction:  [150.6221  184.92693 180.04599 198.74527 139.6775 ]\n",
      "2800: Cose=  2.9462 Prediction:  [150.61037 184.93578 180.0434  198.73569 139.69563]\n",
      "2900: Cose=  2.9256 Prediction:  [150.59901 184.94432 180.04091 198.72615 139.71335]\n",
      "3000: Cose=  2.9055 Prediction:  [150.58806 184.9526  180.03854 198.71677 139.73073]\n",
      "3100: Cose=  2.8859 Prediction:  [150.57747 184.96065 180.03629 198.70746 139.74773]\n",
      "3200: Cose=  2.8667 Prediction:  [150.56726 184.96841 180.03413 198.69826 139.76437]\n",
      "3300: Cose=  2.8479 Prediction:  [150.5574  184.97594 180.03209 198.68916 139.7807 ]\n",
      "3400: Cose=  2.8295 Prediction:  [150.5479  184.98323 180.03017 198.68015 139.79669]\n",
      "3500: Cose=  2.8115 Prediction:  [150.53874 184.99028 180.02832 198.67123 139.81235]\n",
      "3600: Cose=  2.7938 Prediction:  [150.5299  184.9971  180.0266  198.66241 139.8277 ]\n",
      "3700: Cose=  2.7765 Prediction:  [150.5214  185.00372 180.02498 198.65369 139.84274]\n",
      "3800: Cose=  2.7596 Prediction:  [150.5132  185.0101  180.02342 198.64503 139.8575 ]\n",
      "3900: Cose=  2.7429 Prediction:  [150.50528 185.01628 180.02197 198.63647 139.87195]\n",
      "4000: Cose=  2.7265 Prediction:  [150.49767 185.02223 180.02058 198.62796 139.8861 ]\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(0)  # for reproducibility\n",
    "\n",
    "data = np.array([\n",
    "    [73,80,75,152],\n",
    "    [93,88,93,185],\n",
    "    [89,91,90,180],\n",
    "    [96,98,100,196],\n",
    "    [73,66,70,142]\n",
    "])\n",
    "\n",
    "x1_data = data[:,0]\n",
    "x2_data = data[:,1]\n",
    "x3_data = data[:,2]\n",
    "y_data = data[:,3]\n",
    "\n",
    "# placeholders for a tensor that will be always fed\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "for step in range(4001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                          feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print('%4d: Cose=%8.4f' % (step, cost_val), \"Prediction: \", hy_val)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
